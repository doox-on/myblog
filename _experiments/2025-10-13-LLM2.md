---
title: 2. (LLM) NL->JSON->SQL
feature_image: "https://picsum.photos/2560/600?image=1024"
---

During the first two weeks, I focused on understanding the original **NL2SQL agent** from the NORP repository and building the foundation for a new **SQL -> JSON plan generator**.

The goal was to create structured intermediate data that an LLM can later learn from — instead of directly jumping into “NL -> SQL”, I started by **reversing the problem** and teaching the model to reason from existing SQL queries first.

{% include figure.html image="assets/norp2-1.png" caption="Example:SQL -> JSON test" %}

<ul style="font-size: 1.6rem; line-height: 1.6; text-align: center; margin: 0;">
  <li style="list-style: none; margin: 0px 0;">
    <a href="https://gist.github.com/doox-on/2d13b19b8cf428adbaf43d28929b681d" 
       style="font-size: 1.3rem; text-decoration: none;">
      Private Github Repo
    </a>
  </li>
</ul>

---

### Transition to SQL -> JSON Plans

Originally, the plan was to extract query plans from DuckDB via EXPLAIN (FORMAT JSON),
but it did not support that feature yet.
I replaced it with an **LLM based JSON planner** using Groq’s API. 

This new pipeline takes in an SQL query and produces a **structured JSON plan** that outlines:
- The sequence of operations (Projection, Filter, Sort, Join, etc.)
- Their conditions or parameters
- The hierarchical structure of the query


{% include figure.html image="assets/norp2-2.png" caption="Input SQL query" %}

{% include figure.html image="assets/norp2-3.png" caption="Output JSON query" %}


---

### Why This Step Matters

This stage isn’t about fancy UI or multimodal input, it’s about creating a reliable internal structure.
By giving the LLM a consistent JSON schema, we make its reasoning interpretable, verifiable, and trainable.

This step also mirrors the motivation behind NL2Weld:
the idea that an intermediate structured representation (IR) helps bridge natural language and executable logic.

---



### Next Steps (Week 3~4)

- Automatically generate (NL, SQL, JSON) triples using the working SQL -> JSON converter.
- Create ~500 high quality examples to train the NL -> JSON model.
- Fine-tune a lightweight Groq/Llama model to generate JSON plans directly from natural language queries.
- Compare outputs with ground-truth plans to measure semantic alignment (Plan Accuracy).
  
---

### Technology Stack

* **LLM Integration** – Groq API (production model: `llama-3.1-8b-instant`)  
* **Prompting Techniques** – ReAct pipeline for query decomposition + refinement  
* **Database** – SQLite with generated tables from CSV data  
* **Evaluation** – SQL -> JSON
* **Supporting Tools** – data augmentation with paraphrased NLQs  

---
