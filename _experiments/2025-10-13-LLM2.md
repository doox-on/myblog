---
title: 2. (LLM) NL->JSON->SQL
feature_image: "https://picsum.photos/2560/600?image=1024"
---


Based on feedback from a TA and professor’s direction, the project direction was revised from the earlier multimodal NL2SQL prototype toward a more research focused exploration of intermediate representations for query generation and optimization. 
Following ‘NL2Weld’ insight, the project now focuses on building a JSON based execution plan representation that captures query structure elements. Instead of generating SQL directly, the LLM first produces this structured JSON plan, which is then translated into SQL. This approach emphasizes semantic clarity, execution efficiency, and interpretability while aligning with the instructor’s recommendation to explore IR driven model reasoning.


---

### Overview

During the first two weeks, the focus is on building the data processing backbone that connects natural language queries, SQL code, and their corresponding JSON execution plans.
This foundation will later support model fine-tuning and performance evaluation.

1. **DuckDB Integration** A lightweight in-memory database is used to run SQL queries and automatically extract execution plans in JSON format.
2. **SQL->JSON Conversion Utility** Each SQL query in data/raw_sql/ is executed in DuckDB, and the resulting plan is saved under data/json_plans/ for further processing.
3. **Dataset Preparation** Create (NL, SQL, JSON) triples that will serve as the training and evaluation dataset for the model.
4. **Baseline & API Connection** This step verifies that the LLM is responsive and capable of structured generation before fine-tuning begins.

---

### Running the Agent

When I ran `agent.py`, the pipeline:  

- Took in my NLQ.  
- Decomposed it into **sub-queries** with reasoning steps.  
- Executed each sub-query on the SQLite database.  
- Re-assembled the results into a final SQL query.  

It even showed intermediate outputs, so I could trace *why* the model built the final SQL the way it did.  

{% include figure.html image="assets/norp_sql.png" title="Example: NLQ decomposed into SQL sub-queries" %}

```txt

```

---

### Why this was interesting

This wasn’t just a “toy demo” — it was a full pipeline connecting **LLMs → reasoning steps → SQL → execution → evaluation metrics**.  
I could clearly see the difference between *naive SQL generation* and the more structured **ReAct-style prompting** that breaks down queries step by step.  

It gave me a better sense of how LLMs can be applied in real database interfaces, and why evaluation frameworks are critical for moving beyond simple demos.  

---

### Technology Stack

* **LLM Integration** – Groq API (production model: `llama-3.1-8b-instant`)  
* **Prompting Techniques** – ReAct pipeline for query decomposition + refinement  
* **Database** – SQLite with generated tables from CSV data  
* **Evaluation** – structural/output similarity metrics, execution match, exact match  
* **Supporting Tools** – data augmentation with paraphrased NLQs  

---
