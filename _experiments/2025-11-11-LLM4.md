---
title: 4. LLM(evaluation)
feature_image: "https://picsum.photos/2560/600?image=1024"
---

This week’s focus was on evaluating and comparing fine-tuned models across different translation stages.

After training separate NL->JSON, JSON->SQL, and direct NL->SQL models using 500 triplets, each was tested using the same evaluation pipeline.
The results showed that fine tuned models generally outperformed the Groq baseline, though the direct NL->SQL model achieved slightly higher accuracy due to overfitting to familiar data.
This highlighted the need to generate a new, unseen dataset for fair generalization testing in the upcoming phase.

<ul style="font-size: 1.6rem; line-height: 1.6; text-align: center; margin: 0;">
  <li style="list-style: none; margin: 0px 0;">
    <a href="https://github.com/doox-on/CS4220_NORP" 
       style="font-size: 1.3rem; text-decoration: none;">
      Main Project Github Repo
    </a>
  </li>
</ul>

---


### 1. Model Fine Tuning with Llama 3.1

I fine tuned the models using the “meta-llama/Llama-3.1-8b” base model. Each was trained on 500 structured triplets to form a complete NL->JSON->SQL reasoning pipeline.

{% include figure.html image="assets/norp3-1.png" caption="Automated SQL generation with type inference" %}


This approach allowed the model to learn structured reasoning patterns rather than just memorizing syntax. 

---

### 2. NL->JSON->SQL Evaluation Results

The NL->JSON->SQL pipeline showed higher accuracy than the Groq NL->SQL baseline.
However, the direct NL->SQL fine tuned model achieved slightly better scores on the same dataset!
Probably due to overfitting, since it was trained and tested on identical data samples.


{% include figure.html image="assets/norp3-2.png" caption="SQL to JSON execution plan" %}

{% include figure.html image="assets/norp3-2-1.png" caption="500 data set" %}

These results indicate that while the intermediate JSON step improves reasoning consistency,
a new unseen dataset is needed to fairly measure its true generalization benefits.

---




### Next Steps (Week 5~6)

- Create a new unseen dataset to evaluate whether the NL->JSON->SQL pipeline generalizes beyond the training data.
- Conduct a structured comparison between Groq NL->SQL, NL->SQL, and NL->JSON->SQL to measure true performance differences.
- Analyze reasoning behavior to determine if the JSON intermediate step contributes to more interpretable and consistent SQL generation.
  
