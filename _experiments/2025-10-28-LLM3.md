---
title: 3. (LLM) NL-JSON-SQL triplet
feature_image: "https://picsum.photos/2560/600?image=1024"
---

This weekâ€™s focus was on **building the complete NLâ€“JSONâ€“SQL pipeline**, transforming individual experimental scripts into a coherent, reproducible workflow.

After finalizing the **SQL â†’ JSON plan generator**, the next milestones were to:
1. Automatically generate SQL queries from real CSV data,  
2. Use the Groq API to produce their corresponding **natural language (NL)** and **execution plan (JSON)**, and  
3. Merge all valid outputs into a **structured training dataset** for future NL2SQL fine-tuning.


{% include figure.html image="assets/norp2-1.png" caption="Example:SQL -> JSON test" %}

<ul style="font-size: 1.6rem; line-height: 1.6; text-align: center; margin: 0;">
  <li style="list-style: none; margin: 0px 0;">
    <a href="https://github.com/doox-on/CS4220_NORP" 
       style="font-size: 1.3rem; text-decoration: none;">
      Main Project Github Repo
    </a>
  </li>
</ul>

---

### Why This Step Is Important

Most NL2SQL models struggle when trained directly on (NL, SQL) pairs because SQL syntax is symbolic and non-linear â€”  
the model can produce correct-looking but *logically invalid* queries.  

By introducing a **JSON-based intermediate representation**, this project builds a structured "reasoning layer" between natural language and executable code.  
The JSON plan explicitly represents operations like **projection, filtering, grouping, and aggregation**, which helps the model learn *how* a query works, not just *what* it looks like.

In other words:
- The **NL â†’ JSON** step teaches *semantic understanding* (what the user is asking for).  
- The **JSON â†’ SQL** step teaches *logical composition* (how to express that intent as executable code).  

This layered approach mirrors how human data analysts reason â€” by breaking down a question into structured steps before writing code â€”  
making the modelâ€™s reasoning process interpretable, modular, and verifiable.


---

### Transition to SQL -> JSON Plans

Originally, the plan was to extract query plans from DuckDB via EXPLAIN (FORMAT JSON),
but it did not support that feature yet.
I replaced it with an **LLM based JSON planner** using Groqâ€™s API. 

This new pipeline takes in an SQL query and produces a **structured JSON plan** that outlines:
- The sequence of operations (Projection, Filter, Sort, Join, etc.)
- Their conditions or parameters
- The hierarchical structure of the query


---

### 1. Automated SQL Query Generation

I implemented a dynamic SQL generator (`random_query.py`) that scans any CSV file, infers column types (string, int, float), and generates 500 random SQL queries based on adaptive templates.


{% include figure.html image="assets/norp3-1.png" caption="Automated SQL generation with type inference" %}


This enables dataset-independent query generation â€” no manual schema definition required.


---

### 2. SQL â†’ JSON Plan Conversion

The SQL-to-JSON step was stabilized using Groqâ€™s `llama-3.1-8b-instant` model.  
Each SQL query is parsed into a **hierarchical execution plan** with explicit nodes like:
- `TableScan`, `Filter`, `Aggregation`, `Projection`, and `GroupBy`.

{% include figure.html image="assets/norp3-2.png" caption="SQL to JSON execution plan" %}

{% include figure.html image="assets/norp3-2-1.png" caption="500 data set" %}

Invalid or malformed JSON responses are filtered automatically, ensuring only clean data propagates to the final dataset.


---

### ðŸ’¬ 3. SQL â†’ NL Question Generation

A parallel module (`SQL-to-NL.py`) was added to translate SQL queries into plain English questions.  
This process leverages schema-aware prompts (column descriptions embedded in context) to produce accurate, human-like phrasing.

{% include figure.html image="assets/norp3-3.png" caption="SQL to Natural Language generation" %}


By grounding the LLM with column descriptions, the output is both semantically correct and interpretable.


---

### ðŸ”— 4. Triplet Integration

After both NL and JSON files were generated, I built a **triplet merger** (`make_triplets.py`) to align (NL, SQL, JSON) into a unified `.jsonl` dataset.  
This script automatically validates JSON structure and skips corrupted entries.

{% include figure.html image="assets/norp3-4.png" caption="Merged NLâ€“JSONâ€“SQL Triplet Dataset" %}

---



### Next Steps (Week 5~6)

- Automatically generate (NL, SQL, JSON) triples using the working SQL -> JSON converter.
- Create ~500 high quality examples to train the NL -> JSON model.
- Fine-tune a lightweight Groq/Llama model to generate JSON plans directly from natural language queries.
- Compare outputs with ground-truth plans to measure semantic alignment (Plan Accuracy).
  
