---
title: Natural Language → SQL with LLMs (NORP Project)
feature_image: "https://picsum.photos/2560/600?image=1024"
---

For one of my reproducibility assignments, I explored how Large Language Models (LLMs) can **translate natural language questions into SQL queries**.  
The project repo provided two pipelines: one using the **Groq API** with real-time LLM calls, and another designed for a **locally fine-tuned model** with LoRA and ReAct prompting.  

I focused on the API-based pipeline: giving it a question like  
> *“For each zipcode, give the change in average house rent from 2019 to 2022.”*  
and watching the system break it into smaller SQL sub-queries, run them on a SQLite database, and then compose a final SQL query.  

It was surprisingly satisfying to see a plain English sentence get turned into a valid SQL statement that actually ran against the database.  

---

### Overview

The repo is structured with multiple parts:  

1. **Data augmentation** scripts (`augment_ods.py`) that paraphrase queries to make the dataset more robust.  
2. **SQLite utilities** (`pull_table_csv.py`, `sqlite_maker.py`) to build the database from CSV files.  
3. **Evaluation scripts** (`query_tester.py`, `query_comparator.py`) that compare predicted SQL queries with the ground truth using metrics like *Exact Match, Execution Match, Structural Similarity, and Output Similarity*.  
4. **Agents**:  
   - `agent.py` → connects to the Groq API, decomposes natural language queries, and synthesizes final SQL queries.  
   - `NL2SQL_finetuned.py` → (optional) a locally fine-tuned Llama model with LoRA and ReAct reasoning loops.  

---

### Running the Agent

When I ran `agent.py`, the pipeline:  

- Took in my NLQ.  
- Decomposed it into **sub-queries** with reasoning steps.  
- Executed each sub-query on the SQLite database.  
- Re-assembled the results into a final SQL query.  

It even showed intermediate outputs, so I could trace *why* the model built the final SQL the way it did.  

{% include figure.html image="assets/norp_sql.png" title="Example: NLQ decomposed into SQL sub-queries" %}

---

### Evaluation

To test accuracy, I used `query_tester.py`.  
By default, it has a **dummy SQL generator** (`SELECT * FROM <table>`) which gave predictably low scores (Exact Match = 0, Structural Similarity ≈ 0.12).  
Still, the evaluation framework worked as expected, showing how well generated SQL queries can be measured against real ground truth.  

{% include figure.html image="assets/query_tester.png" title="query_tester.py" %}
---

### Why this was interesting

This wasn’t just a “toy demo” — it was a full pipeline connecting **LLMs → reasoning steps → SQL → execution → evaluation metrics**.  
I could clearly see the difference between *naive SQL generation* and the more structured **ReAct-style prompting** that breaks down queries step by step.  

It gave me a better sense of how LLMs can be applied in real database interfaces, and why evaluation frameworks are critical for moving beyond simple demos.  

---

### Technology Stack

* **LLM Integration** – Groq API (production model: `llama-3.1-8b-instant`)  
* **Prompting Techniques** – ReAct pipeline for query decomposition + refinement  
* **Database** – SQLite with generated tables from CSV data  
* **Evaluation** – structural/output similarity metrics, execution match, exact match  
* **Supporting Tools** – data augmentation with paraphrased NLQs  

---
